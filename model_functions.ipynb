{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os;import sys\n",
    "import pandas as pd;import numpy as np;\n",
    "import networkx as nx\n",
    "import numpy.random as npr \n",
    "from scipy.stats.distributions import chi2\n",
    "from scipy.special import gamma, factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDiGraph(log_df):\n",
    "    \"\"\"A function to build a bipartite graph from an action log.\n",
    "    - log_df (dataframe) contains timestamped network links (Source | Target | Timestamp) \"\"\"\n",
    "    \n",
    "    B = nx.DiGraph()\n",
    "    B.add_nodes_from(log_df['Source'].unique(), bipartite=0)\n",
    "    B.add_nodes_from(log_df['Target'].unique(), bipartite=1)\n",
    "    \n",
    "    for i,row in log_df.iterrows():\n",
    "        e, year = (row['Source'],row['Target']), row['Timestamp']\n",
    "        if e in B.edges():\n",
    "            w = B.edges[e]['weight']\n",
    "            B.add_edge(*e,weight=w+1)\n",
    "        else:\n",
    "            B.add_edge(*e,weight=1)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addWeightedEdge(B,e):\n",
    "    b = B.copy()\n",
    "    if e in b.edges():\n",
    "        w = b.edges[e]['weight']\n",
    "        b.add_edge(*e,weight=w+1)\n",
    "    else:\n",
    "        b.add_edge(*e,weight=1)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SIMULATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    \"\"\"\n",
    "    A function to efficiently sample a non-uniform probability distribution using Vose's method:\n",
    "    https://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    \n",
    "    \"\"\"\n",
    "    K       = len(probs)\n",
    "    q       = np.zeros(K)\n",
    "    J       = np.zeros(K, dtype=int)\n",
    "\n",
    "    smaller = []  # Sort the data into the outcomes with probabilities\n",
    "    larger  = []  # that are larger and smaller than 1/K.\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "            \n",
    "    while len(smaller) > 0 and len(larger) > 0: \n",
    "        small = smaller.pop() \n",
    "        large = larger.pop() \n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] - (1.0 - q[small])\n",
    "\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    K  = len(J)\n",
    "\n",
    "    # Draw from the overall uniform mixture.\n",
    "    kk = int(np.floor(npr.rand()*K))\n",
    "\n",
    "    # Draw from the binary mixture, either keeping the\n",
    "    # small one, or choosing the associated larger one.\n",
    "    if npr.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simGraph(b, log_df, source_nodes, target_nodes, feature_df, features): \n",
    "    \"\"\" A function to simulate the 'fixed-effects' network model\n",
    "    wherein the probability of an attachment to a target node is proportional to an exponentiated \n",
    "    utility function: p ~ exp(u_nt) where u = b * f_nt for feature f and node n at time t\n",
    "     - b (list) is the vector of utility parameters \n",
    "     - log (dataframe) contains the timestamped network links ( Source | Target | Timestamp )\n",
    "     - target_nodes (list)\n",
    "     - source_nodes (list)\n",
    "     - feature_df(dataframe) contains the feature variables to be used in the utility function...\n",
    "                                                      ( Country | Year | Feature 1 | ... | Feature N )\n",
    "     - features (list) is a vector of features                                                \n",
    "     Returns the final network B and a dictionary of edges keyed by year.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Initialize the bipartite graph with source nodes and target nodes\n",
    "    B = nx.DiGraph()\n",
    "    B.add_nodes_from(source_nodes,bipartite=0)\n",
    "    B.add_nodes_from(target_nodes,bipartite=1)\n",
    "    \n",
    "    # Initalize a list of edges keyed by year\n",
    "    edges = {y:[] for y in sorted(log_df['Timestamp'].unique())}\n",
    "    \n",
    "    # Create dictionary from feature dataframe indexing the value of feature f by target node n, and year t\n",
    "    f_nt = {f:{n:{row['Year']:row[f] for j, row in feature_df.groupby('Country').get_group(n).iterrows()}\\\n",
    "                 for n in target_nodes} for f in features}\n",
    "    \n",
    "    for i, row in log_df.iterrows():\n",
    "        t, source = row['Timestamp'], row['Source']\n",
    "       \n",
    "        # Create dictionary to contain probability of attachment to each target node n at time t\n",
    "        p = {}\n",
    "        for n in target_nodes:\n",
    "            p[n] = np.exp(np.dot([f_nt[f][n][t] for f in features], b))\n",
    "            \n",
    "        \n",
    "        Z = sum([p[n] for n in target_nodes])\n",
    "        P = [p[n]/Z if Z > 0 else 0 for n in target_nodes] # create normalized list of probabilities\n",
    "        \n",
    "        # Draw the target node from the probability distribution P\n",
    "        J,q = alias_setup(P)\n",
    "        target = target_nodes[alias_draw(J, q)]\n",
    "    \n",
    "        # Add the new edge to the graph\n",
    "        edges[t].append((source,target))\n",
    "        B = addWeightedEdge(B,(source,target))\n",
    "    \n",
    "    return B, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL ESTIMATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logL(b, log_df, target_nodes, feature_df, features):\n",
    "    \"\"\" A function to calucate the loglikelhood of the dataset based on the 'fixed-effects' network model\n",
    "    wherein the probability of an investment is proportional to an exponentiated utility function:\n",
    "    p ~ exp(u_nt) where u = b * f_nt for feature f and country n at time t\n",
    "     - b (list) is parameter vector to be estimated, \n",
    "     - log_df (dataframe) containing the timestamped network links ( Source | Target | Timestamp )\n",
    "     - target_nodes (list)\n",
    "     - feature_df (dataframe) contains feature variables to be tested in the utility function...\n",
    "                                                      ( Country | Year | Feature 1 | ... | Feature N )\"\"\"\n",
    "    \n",
    "    \n",
    "    # Initialize likelihood function L and regularization function S\n",
    "    L = 0\n",
    "    \n",
    "    # Create dictionary from feature dataframe indexing the value of feature f by country n, and year t\n",
    "    f_nt = {f:{n:{row['Year']:row[f] for j, row in feature_df.groupby('Country').get_group(n).iterrows()}\\\n",
    "                 for n in target_nodes} for f in features}\n",
    "    \n",
    "    # Calculate likelihood for each network link\n",
    "    for i, row in log_df.iterrows():\n",
    "        source, target, t = row['Source'], row['Target'], row['Timestamp']\n",
    "        \n",
    "        u = {} # Initialize dictionary to store utility of each country u = b * f_nt\n",
    "        for i,n in enumerate(target_nodes):\n",
    "            u[n] = np.dot([f_nt[f][n][t] for f in features], b)\n",
    "\n",
    "        L += u[target] - np.log(sum([np.exp(u[n]) for n in target_nodes])) \n",
    "                \n",
    "    return L "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X2_p(logL0, logLM, k):\n",
    "    \"\"\" A function to compute the Xi-squared test statistic for the Wald's likelihood ratio test\n",
    "    - logL0 is the loglikelihood of the null model \n",
    "    - logLM is the loglikelihood of the model to be evalulated\n",
    "    - k is the number of degrees of freedom\n",
    "    \"\"\"\n",
    "    X2 = -2*(logL0-logLM)\n",
    "    return chi2.sf(X2,k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
